{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: Multi-class Classification \n",
    "In this Assignment, you will train a deep model on the CIFAR10 from the scratch using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/root/.local/lib/python3.10/site-packages')\n",
    "# !python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --user --upgrade seaborn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "# print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 1 \n",
    "NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 30\n",
    "EVAL_INTERVAL=1\n",
    "SAVE_DIR = './ckpt'\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-1\n",
    "MOMENTUM = 0.9\n",
    "STEP=5\n",
    "GAMMA=0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar10 transform\n",
    "transform_cifar10_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_cifar10_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "                                        download=True, transform=transform_cifar10_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=transform_cifar10_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 4, 3)  \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3)  \n",
    "        self.fc1 = nn.Linear(8 * 6 * 6, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 6 * 6)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, input_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj):\n",
    "        super().__init__()\n",
    "\n",
    "        #1x1conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, n1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        #1x1conv -> 3x3conv branch\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, n3x3_reduce, kernel_size=1),\n",
    "            nn.BatchNorm2d(n3x3_reduce),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(n3x3_reduce, n3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n3x3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        #1x1conv -> 5x5conv branch\n",
    "        #we use 2 3x3 conv filters stacked instead\n",
    "        #of 1 5x5 filters to obtain the same receptive\n",
    "        #field with fewer parameters\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, n5x5_reduce, kernel_size=1),\n",
    "            nn.BatchNorm2d(n5x5_reduce),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(n5x5_reduce, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5, n5x5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        #3x3pooling -> 1x1conv\n",
    "        #same conv\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(input_channels, pool_proj, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_proj),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1)\n",
    "\n",
    "\n",
    "class GoogleNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=10):\n",
    "        super().__init__()\n",
    "        self.prelayer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        #although we only use 1 conv layer as prelayer,\n",
    "        #we still use name a3, b3.......\n",
    "        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "        ##\"\"\"In general, an Inception network is a network consisting of\n",
    "        ##modules of the above type stacked upon each other, with occasional\n",
    "        ##max-pooling layers with stride 2 to halve the resolution of the\n",
    "        ##grid\"\"\"\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        #input feature size: 8*8*1024\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout2d(p=0.4)\n",
    "        self.linear = nn.Linear(1024, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelayer(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.b3(x)\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.a4(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.d4(x)\n",
    "        x = self.e4(x)\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.a5(x)\n",
    "        x = self.b5(x)\n",
    "\n",
    "        #\"\"\"It was found that a move from fully connected layers to\n",
    "        #average pooling improved the top-1 accuracy by about 0.6%,\n",
    "        #however the use of dropout remained essential even after\n",
    "        #removing the fully connected layers.\"\"\"\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def googlenet():\n",
    "    return GoogleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"L1 Loss\": googlenet(),\n",
    "    \"CE Loss\": googlenet(),\n",
    "    \"Focal Loss(γ=0.5)\": googlenet(),\n",
    "    \"Focal Loss(γ=2.0)\": googlenet(),\n",
    "}\n",
    "optimizers = {}\n",
    "schedulers = {}\n",
    "\n",
    "# model = ConvNet()\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    model.to(device)\n",
    "    optimizers[model_name] = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "    schedulers[model_name] = torch.optim.lr_scheduler.StepLR(optimizers[model_name], step_size=STEP, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: per batch training/testing\n",
    "---\n",
    "\n",
    "Please denfine two function named ``train_batch`` and ``test_batch``. These functions are essential for training and evaluating machine learning models using batched data from dataloaders.\n",
    "\n",
    "**To do**: \n",
    "1. Define the loss function i.e [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "2. Take the image as the input and generate the output using the pre-defined SimpleNet.\n",
    "3. Calculate the loss between the output and the corresponding label using the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Write your answer here ##################\n",
    "# Define the loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "criterions = {\n",
    "    \"L1 Loss\": (nn.L1Loss(reduction = \"mean\"), True),\n",
    "    \"CE Loss\": (nn.CrossEntropyLoss(), True),\n",
    "    \n",
    "    \"Focal Loss(γ=0.5)\": (FocalLoss(gamma = 0.5), True),\n",
    "    \"Focal Loss(γ=2.0)\": (FocalLoss(gamma = 2.0), True),\n",
    "}\n",
    "\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, image, target, criterion):\n",
    "    \"\"\"\n",
    "    Perform one training batch iteration.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The machine learning model to train.\n",
    "        image (torch.Tensor): Batch of input data (images).\n",
    "        target (torch.Tensor): Batch of target labels.\n",
    "        criterion (torch.nn.Module)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Model output (predictions) for the batch.\n",
    "        torch.Tensor: Loss value calculated by the defined loss function loss_fn().\n",
    "    \"\"\"\n",
    "    \n",
    "    ##################### Write your answer here ##################\n",
    "    model.train()\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "    ###############################################################\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_batch(model, image, target, criterion):\n",
    "    \"\"\"\n",
    "    Perform one testing batch iteration.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The machine learning model to evaluate.\n",
    "        image (torch.Tensor): Batch of input data (images).\n",
    "        target (torch.Tensor): Batch of target labels.\n",
    "        criterion (torch.nn.Module)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Model output (predictions) for the batch.\n",
    "        torch.Tensor: Loss value calculated for the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    ##################### Write your answer here ##################\n",
    "    model.eval()\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "    ###############################################################\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_histories = {}\n",
    "train_acc_histories = {}\n",
    "test_loss_histories = {}\n",
    "test_acc_histories = {}\n",
    "test_precision_histories = {}\n",
    "test_recall_histories = {}\n",
    "test_f1_histories = {}\n",
    "test_targets_histories = {}\n",
    "test_probs_histories = {}\n",
    "test_preds_histories = {}\n",
    "kappas = {}\n",
    "train_times = {}\n",
    "test_times = {}\n",
    "\n",
    "\n",
    "read_ckpt = True  # True -> enable to load trained models\n",
    "\n",
    "\n",
    "for criterion in criterions:\n",
    "    if not criterions[criterion][1]:\n",
    "        continue\n",
    "    print(criterion)\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    test_loss_history = []\n",
    "    test_acc_history = []\n",
    "    test_precision_history = []\n",
    "    test_recall_history = []\n",
    "    test_f1_history = []\n",
    "    test_targets_history = []\n",
    "    test_probs_history = []\n",
    "    test_preds_history = []\n",
    "    test_kappas = []\n",
    "    train_time = []\n",
    "    test_time = []\n",
    "    \n",
    "    t1 = time.time()\n",
    "    \n",
    "    model = models[criterion]\n",
    "    optimizer = optimizers[criterion]\n",
    "    scheduler = schedulers[criterion]\n",
    "    \n",
    "    ckpt_pth = osp.join(SAVE_DIR, f\"{criterion}_checkpoint_{(str(NUM_EPOCHS))}.pth\")\n",
    "\n",
    "    if os.path.exists(ckpt_pth) and read_ckpt:\n",
    "        print(ckpt_pth)\n",
    "        ckpt = torch.load(ckpt_pth)\n",
    "        model.load_state_dict(ckpt[\"state_dict\"]) \n",
    "        model.eval()\n",
    "        continue\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        ##########################\n",
    "        ### Training\n",
    "        ##########################\n",
    "\n",
    "        running_cls_loss = 0.0\n",
    "        running_cls_corrects = 0\n",
    "\n",
    "        for batch_idx, (image, target) in enumerate(train_dataloader):\n",
    "\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # train model\n",
    "            if criterion != \"L1 Loss\":\n",
    "                outputs, loss = train_batch(model, image, target, criterions[criterion][0])\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "            else:\n",
    "                outputs = model(image)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterions[criterion][0](\n",
    "                    F.softmax(outputs, dim = 1), \n",
    "                    F.one_hot(target, num_classes = 10)\n",
    "                )\n",
    "            loss_data = loss.data.item()\n",
    "            if np.isnan(loss_data):\n",
    "                raise ValueError('loss is nan while training')\n",
    "            running_cls_loss += loss.item()\n",
    "            running_cls_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss = running_cls_loss / len(train_set)\n",
    "        epoch_acc = running_cls_corrects.double() / len(train_set)\n",
    "\n",
    "        print(f'Epoch: {epoch+1}/{NUM_EPOCHS} Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        train_acc_history.append(epoch_acc.cpu().detach().numpy())\n",
    "        \n",
    "        # change learning rate\n",
    "        scheduler.step()\n",
    "#         scheduler.\n",
    "        \n",
    "        t2 = time.time()\n",
    "        train_time.append(t2 - t1)\n",
    "\n",
    "        train_acc = accuracy_score(target.cpu().numpy(), preds.cpu().numpy())\n",
    "        print(f'Train acc: {train_acc:.4f} Epoch_acc: {epoch_acc:.4f} Epoch loss: {epoch_loss:.4f}')\n",
    "\n",
    "        ##########################\n",
    "        ### Testing\n",
    "        ##########################\n",
    "        # # eval model during training or in the last epoch\n",
    "\n",
    "        test_preds = []  \n",
    "        test_targets = [] \n",
    "        test_probs = [] \n",
    "        if (epoch + 1) % EVAL_INTERVAL == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "            print('Begin test......')\n",
    "            model.eval()\n",
    "\n",
    "            val_loss = 0.0\n",
    "            val_corrects = 0\n",
    "\n",
    "            for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "\n",
    "                image = image.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # test model\n",
    "                if criterion != \"L1 Loss\":\n",
    "                    outputs, loss = test_batch(model, image, target, criterions[criterion][0])\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                else:\n",
    "                    outputs = model(image)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterions[criterion][0](\n",
    "                        F.softmax(outputs, dim = 1), \n",
    "                        F.one_hot(target, num_classes = 10)\n",
    "                    )\n",
    "                val_loss += loss.item()\n",
    "                val_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_targets.extend(target.cpu().numpy())\n",
    "                test_probs.extend(outputs.cpu().detach().numpy())\n",
    "                \n",
    "                \n",
    "            val_loss = val_loss / len(test_set)\n",
    "            val_acc = val_corrects.double() / len(test_set)\n",
    "            test_precision = precision_score(test_targets, test_preds, average='weighted')\n",
    "            test_recall = recall_score(test_targets, test_preds, average='weighted')\n",
    "            test_f1 = f1_score(test_targets, test_preds, average='weighted')\n",
    "\n",
    "            print(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f} Precision: {test_precision:.4f} Recall: {test_recall:.4f} F1-Score: {test_f1:.4f}')\n",
    "\n",
    "            test_loss_history.append(val_loss)\n",
    "            test_acc_history.append(val_acc.cpu().detach().numpy())\n",
    "            test_precision_history.append(test_precision)\n",
    "            test_recall_history.append(test_recall)\n",
    "            test_f1_history.append(test_f1)\n",
    "            test_kappas.append(cohen_kappa_score(test_targets, test_preds))\n",
    "\n",
    "            # save the model in last epoch\n",
    "            if (epoch + 1) == NUM_EPOCHS:\n",
    "\n",
    "                state = {\n",
    "                'state_dict': model.state_dict(),\n",
    "                'acc': epoch_acc,\n",
    "                'epoch': (epoch+1),\n",
    "                }\n",
    "\n",
    "                # check the dir\n",
    "                if not os.path.exists(SAVE_DIR):\n",
    "                    os.makedirs(SAVE_DIR)\n",
    "\n",
    "                # save the state\n",
    "                torch.save(state, osp.join(SAVE_DIR, f\"{criterion}_checkpoint_{(str(epoch+1))}.pth\"))\n",
    "        \n",
    "        t1 = time.time()\n",
    "        test_time.append(t1 - t2)\n",
    "    \n",
    "    train_loss_histories[criterion] = train_loss_history\n",
    "    train_acc_histories[criterion] = train_acc_history\n",
    "    test_loss_histories[criterion] = test_loss_history\n",
    "    test_acc_histories[criterion] = test_acc_history\n",
    "    test_precision_histories[criterion] = test_precision_history\n",
    "    test_recall_histories[criterion] = test_recall_history\n",
    "    test_f1_histories[criterion] = test_f1_history\n",
    "    test_targets_histories[criterion] = test_targets\n",
    "    test_probs_histories[criterion] = test_probs\n",
    "    test_preds_histories[criterion] = test_preds\n",
    "    kappas[criterion] = test_kappas\n",
    "    train_times[criterion] = train_time\n",
    "    test_times[criterion] = test_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Instance inference\n",
    "---\n",
    "The task is to visualizes an image along with model prediction and class probabilities.\n",
    "\n",
    "**To do**: \n",
    "1. Calculate the prediction and the probabilities for each class.\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(test_dataloader))\n",
    "input = inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Write your answer here ##################\n",
    "# input: image, model\n",
    "# outputs: predict_label, probabilities\n",
    "# predict_label is the index (or label) of the class with the highest probability from the probabilities.\n",
    "###############################################################\n",
    "\n",
    "input = input.to(device)\n",
    "model = model.cuda()\n",
    "\n",
    "probabilities = F.softmax(model(input.unsqueeze(0)), dim = 1).squeeze(0)\n",
    "predict_label = torch.argmax(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model(input.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = class_names[predict_label.item()]\n",
    "predicted_probability = probabilities[predict_label].item()\n",
    "image = input.cpu().numpy().transpose((1, 2, 0))\n",
    "plt.imshow(image)\n",
    "plt.text(17, 30, f'Predicted Class: {predicted_class}\\nProbability: {predicted_probability:.2f}', \n",
    "            color='white', backgroundcolor='black', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# Print probabilities for each class\n",
    "print('Print probabilities for each class:')\n",
    "for i in range(len(class_names)):\n",
    "    print(f'{class_names[i]}: {probabilities[i].item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_dict_to_csv(filename, data_dict):\n",
    "    with open(filename, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        \n",
    "        header = data_dict.keys()\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        rows = zip(*data_dict.values())\n",
    "        writer.writerows(rows)\n",
    "            \n",
    "def read_csv_to_dict(filename, flt = True):\n",
    "    data_dict = {}\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        print(filename)\n",
    "        for row in reader:\n",
    "            for key, value in row.items():\n",
    "                if key not in data_dict:\n",
    "                    data_dict[key] = []\n",
    "                if flt:\n",
    "                    data_dict[key].append(float(value))\n",
    "                else:\n",
    "                    numbers_str = value.strip('[]').split()\n",
    "                    numbers_list = [float(num) for num in numbers_str]\n",
    "                    data_dict[key].append(numbers_list)\n",
    "    return data_dict\n",
    "\n",
    "save_dict = False\n",
    "if not read_ckpt or save_dict:\n",
    "    save_dict_to_csv(f'train_loss_histories{NUM_EPOCHS}.csv', train_loss_histories)\n",
    "    save_dict_to_csv(f'train_acc_histories{NUM_EPOCHS}.csv', train_acc_histories)\n",
    "    save_dict_to_csv(f'test_loss_histories{NUM_EPOCHS}.csv', test_loss_histories)\n",
    "    save_dict_to_csv(f'test_acc_histories{NUM_EPOCHS}.csv', test_acc_histories)\n",
    "    save_dict_to_csv(f'test_precision_histories{NUM_EPOCHS}.csv', test_precision_histories)\n",
    "    save_dict_to_csv(f'test_recall_histories{NUM_EPOCHS}.csv', test_recall_histories)\n",
    "    save_dict_to_csv(f'test_f1_histories{NUM_EPOCHS}.csv', test_f1_histories)\n",
    "    save_dict_to_csv(f'test_targets_histories{NUM_EPOCHS}.csv', test_targets_histories)\n",
    "    save_dict_to_csv(f'test_probs_histories{NUM_EPOCHS}.csv', test_probs_histories)\n",
    "    save_dict_to_csv(f'test_preds_histories{NUM_EPOCHS}.csv', test_preds_histories)\n",
    "    save_dict_to_csv(f'kappas{NUM_EPOCHS}.csv', kappas)\n",
    "    save_dict_to_csv(f'train_times{NUM_EPOCHS}.csv', train_times)\n",
    "    save_dict_to_csv(f'test_times{NUM_EPOCHS}.csv', test_times)\n",
    "else:\n",
    "    train_loss_histories = read_csv_to_dict(f'train_loss_histories{NUM_EPOCHS}.csv')\n",
    "    train_acc_histories = read_csv_to_dict(f'train_acc_histories{NUM_EPOCHS}.csv')\n",
    "    test_loss_histories = read_csv_to_dict(f'test_loss_histories{NUM_EPOCHS}.csv')\n",
    "    test_acc_histories = read_csv_to_dict(f'test_acc_histories{NUM_EPOCHS}.csv')\n",
    "    test_precision_histories = read_csv_to_dict(f'test_precision_histories{NUM_EPOCHS}.csv')\n",
    "    test_recall_histories = read_csv_to_dict(f'test_recall_histories{NUM_EPOCHS}.csv')\n",
    "    test_f1_histories = read_csv_to_dict(f'test_f1_histories{NUM_EPOCHS}.csv')\n",
    "    test_targets_histories = read_csv_to_dict(f'test_targets_histories{NUM_EPOCHS}.csv')\n",
    "    test_probs_histories = read_csv_to_dict(f'test_probs_histories{NUM_EPOCHS}.csv', False)\n",
    "    test_preds_histories = read_csv_to_dict(f'test_preds_histories{NUM_EPOCHS}.csv', False)\n",
    "    kappas = read_csv_to_dict(f'kappas{NUM_EPOCHS}.csv')\n",
    "    train_times = read_csv_to_dict(f'train_times{NUM_EPOCHS}.csv')\n",
    "    test_times = read_csv_to_dict(f'test_times{NUM_EPOCHS}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_histories = {}\n",
    "# train_acc_histories = {}\n",
    "# test_loss_histories = {}\n",
    "# test_acc_histories = {}\n",
    "# test_precision_histories = {}\n",
    "# test_recall_histories = {}\n",
    "# test_f1_histories = {}\n",
    "\n",
    "x = NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "sys.path.append('/data/root/.local/lib/python3.10/site-packages')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --user --upgrade seaborn\n",
    "\n",
    "# sys.path\n",
    "# get_ipython().system(\n",
    "#     \"export PYTHONPATH=/data/root/.local/lib/python3.10/site-packages:/usr/local/lib/python3.10/dist-packages:$PYTHONPATH\"\n",
    "# )\n",
    "# !which python\n",
    "# !jupyter kernelspec list\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, NUM_EPOCHS + 1)\n",
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for legend, y in train_acc_histories.items():\n",
    "    plt.plot(x, y, label = legend)\n",
    "plt.xlabel(\"#Epoch\")\n",
    "plt.ylabel(\"Train accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for legend, y in train_loss_histories.items():\n",
    "    plt.plot(x, y, label = legend)\n",
    "plt.xlabel(\"#Epoch\")\n",
    "plt.ylabel(\"Train loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for legend, y in test_acc_histories.items():\n",
    "    plt.plot(x, y, label = legend)\n",
    "plt.xlabel(\"#Epoch\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for legend, y in test_loss_histories.items():\n",
    "    plt.plot(x, y, label = legend)\n",
    "plt.xlabel(\"#Epoch\")\n",
    "plt.ylabel(\"Test loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for legend, y in test_precision_histories.items():\n",
    "    plt.plot(x, y, label = legend)\n",
    "plt.xlabel(\"#Epoch\")\n",
    "plt.ylabel(\"Test precision\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for legend, y in test_recall_histories.items():\n",
    "    plt.plot(x, y, label = legend)\n",
    "plt.xlabel(\"#Epoch\")\n",
    "plt.ylabel(\"Test recall\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for legend, y in test_f1_histories.items():\n",
    "    plt.plot(x, y, label = legend)\n",
    "plt.xlabel(\"#Epoch\")\n",
    "plt.ylabel(\"Test f1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for legend, y in kappas.items():\n",
    "    plt.plot(x, y, label = legend)\n",
    "plt.xlabel(\"#Epoch\")\n",
    "plt.ylabel(\"Kappas Statistics\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for legend, y in test_times.items():\n",
    "    plt.plot(x, y, label = legend)\n",
    "plt.xlabel(\"#Epoch\")\n",
    "plt.ylabel(\"Test time(sec)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for legend, y in train_times.items():\n",
    "    plt.plot(x, y, label = legend)\n",
    "plt.xlabel(\"#Epoch\")\n",
    "plt.ylabel(\"Train time(sec)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(6, 20), dpi = 300)\n",
    "place_map = {\n",
    "    \"L1 Loss\": 0,\n",
    "    \"CE Loss\": 1,\n",
    "    \"Focal Loss(γ=0.5)\": 2,\n",
    "    \"Focal Loss(γ=2.0)\": 3,\n",
    "}\n",
    "\n",
    "kappas = {}\n",
    "\n",
    "for criterion in test_targets_histories:\n",
    "    test_targets = test_targets_histories[criterion]\n",
    "    test_preds = test_preds_histories[criterion]\n",
    "    confusion = confusion_matrix(test_targets, test_preds)\n",
    "\n",
    "    px = place_map[criterion]\n",
    "    py = 0\n",
    "    sns.heatmap(confusion, annot=True, cmap=\"Blues\", fmt='d', ax=ax[px])\n",
    "    ax[px].set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "    ax[px].set_yticklabels(class_names, rotation=0)\n",
    "    ax[px].set_title(f'Confusion Matrix of {criterion}')\n",
    "    ax[px].set_xlabel('Predicted')\n",
    "    ax[px].set_ylabel('True')\n",
    "    \n",
    "    kappas[criterion] = cohen_kappa_score(test_targets, test_preds)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.rcParams['agg.path.chunksize'] = 0 \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "plt.bar(list(kappas.keys()), list(kappas.values()))\n",
    "plt.title(\"Kappa Statistics\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
