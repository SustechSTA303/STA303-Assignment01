{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: Multi-class Classification \n",
    "In this Assignment, you will train a deep model on the CIFAR10 from the scratch using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 1 \n",
    "NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 30\n",
    "EVAL_INTERVAL=1\n",
    "SAVE_DIR = './log'\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-1\n",
    "MOMENTUM = 0.9\n",
    "STEP=5\n",
    "GAMMA=0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# cifar10 transform\n",
    "transform_cifar10_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_cifar10_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "                                        download=True, transform=transform_cifar10_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=transform_cifar10_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 4, 3)  \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3)  \n",
    "        self.fc1 = nn.Linear(8 * 6 * 6, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 6 * 6)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=288, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: per batch training/testing\n",
    "---\n",
    "\n",
    "Please denfine two function named ``train_batch`` and ``test_batch``. These functions are essential for training and evaluating machine learning models using batched data from dataloaders.\n",
    "\n",
    "**To do**: \n",
    "1. Define the loss function i.e [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "2. Take the image as the input and generate the output using the pre-defined SimpleNet.\n",
    "3. Calculate the loss between the output and the corresponding label using the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return torch.mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Write your answer here ##################\n",
    "# Define the loss function\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "criterion3 = FocalLoss(gamma=0.5)\n",
    "criterion4 = FocalLoss(gamma=2)\n",
    "criterion  = nn.L1Loss()\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, image, target):\n",
    "    output = model(image)\n",
    "    loss =  criterion(output, target)\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, image, target):\n",
    "    output =  model(image)\n",
    "    loss =   criterion(output, target)\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# train model 这里是修改train_batch1234的地方\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m outputs, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m loss_data \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(model, image, target)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_batch\u001b[39m(model, image, target):\n\u001b[1;32m      2\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[0;32m----> 3\u001b[0m     loss \u001b[38;5;241m=\u001b[39m  \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output, loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:101\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3299\u001b[0m, in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3297\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3299\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39ml1_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "training_loss = []\n",
    "training_acc = []\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ##########################\n",
    "    ### Training\n",
    "    ##########################\n",
    "\n",
    "    running_cls_loss = 0.0\n",
    "    running_cls_corrects = 0\n",
    "\n",
    "    for batch_idx, (image, target) in enumerate(train_dataloader):\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # train model 这里是修改train_batch1234的地方\n",
    "        outputs, loss = train_batch(model, image, target)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        \n",
    "        loss_data = loss.data.item()\n",
    "        if np.isnan(loss_data):\n",
    "            raise ValueError('loss is nan while training')\n",
    "        running_cls_loss += loss.item()\n",
    "        running_cls_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = running_cls_loss / len(train_set)\n",
    "    epoch_acc = running_cls_corrects.double() / len(train_set)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    training_loss.append(epoch_loss)\n",
    "    training_acc.append(epoch_acc.cpu().detach().numpy())\n",
    "\n",
    "    # change learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    ### Testing\n",
    "    ##########################\n",
    "    # # eval model during training or in the last epoch\n",
    "    if (epoch + 1) % EVAL_INTERVAL == 0 or (epoch +1) == NUM_EPOCHS:\n",
    "        print('Begin test......')\n",
    "        model.eval()\n",
    "    \n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # test model\n",
    "            outputs, loss = test_batch(model, image, target)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_set)\n",
    "        val_acc = val_corrects.double() / len(test_set)\n",
    "        print(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        testing_loss.append(val_loss)\n",
    "        testing_acc.append(val_acc.cpu().detach().numpy())\n",
    "\n",
    "        # save the model in last epoch\n",
    "        if (epoch +1) == NUM_EPOCHS:\n",
    "            \n",
    "            state = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': epoch_acc,\n",
    "            'epoch': (epoch+1),\n",
    "            }\n",
    "\n",
    "            # check the dir\n",
    "            if not os.path.exists(SAVE_DIR):\n",
    "                os.makedirs(SAVE_DIR)\n",
    "\n",
    "            # save the state\n",
    "            torch.save(state, osp.join(SAVE_DIR, 'checkpoint_%s.pth' % (str(epoch+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Instance inference\n",
    "---\n",
    "The task is to visualizes an image along with model prediction and class probabilities.\n",
    "\n",
    "**To do**: \n",
    "1. Calculate the prediction and the probabilities for each class.\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(test_dataloader))\n",
    "input = inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Write your answer here ##################\n",
    "# input: image, model\n",
    "# outputs: predict_label, probabilities\n",
    "# predict_label is the index (or label) of the class with the highest probability from the probabilities.\n",
    "###############################################################\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "input = input.to(device)\n",
    "probabilities = F.softmax(model(input)[0],dim=0)\n",
    "predict_label = torch.argmax(probabilities, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print probabilities for each class:\n",
      "airplane: 0.0057\n",
      "automobile: 0.0038\n",
      "bird: 0.0287\n",
      "cat: 0.1439\n",
      "deer: 0.0120\n",
      "dog: 0.0189\n",
      "frog: 0.7720\n",
      "horse: 0.0009\n",
      "ship: 0.0094\n",
      "truck: 0.0048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predicted_class = class_names[predict_label.item()]\n",
    "predicted_probability = probabilities[predict_label].item()\n",
    "image = input.cpu().numpy().transpose((1, 2, 0))\n",
    "plt.imshow(image)\n",
    "plt.text(17, 30, f'Predicted Class: {predicted_class}\\nProbability: {predicted_probability:.2f}', \n",
    "            color='white', backgroundcolor='black', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# Print probabilities for each class\n",
    "print('Print probabilities for each class:')\n",
    "for i in range(len(class_names)):\n",
    "    print(f'{class_names[i]}: {probabilities[i].item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
