{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: Multi-class Classification \n",
    "In this Assignment, you will train a deep model on the CIFAR10 from the scratch using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 1 \n",
    "NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 300\n",
    "EVAL_INTERVAL=1\n",
    "SAVE_DIR = './log'\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "STEP=5\n",
    "GAMMA=0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# cifar10 transform\n",
    "transform_cifar10_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),#为使用googleNet特有\n",
    "    #transforms.RandomCrop(32, padding=4),为保持形状\n",
    "    transforms.RandomResizedCrop(224),  # 随机裁剪图片为224x224\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_cifar10_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),#为使用googleNet特有\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "                                        download=True, transform=transform_cifar10_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=transform_cifar10_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 4, 3)  \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3)  \n",
    "        self.fc1 = nn.Linear(8 * 6 * 6, 32)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 6 * 6)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\"\"\"    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu3(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        \n",
    "        x = self.relu4(self.dropout1(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
    "        super(Inception, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            nn.Conv2d(ch3x3red, ch3x3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            nn.Conv2d(ch5x5red, ch5x5, kernel_size=5, padding=2),\n",
    "        )\n",
    "\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, pool_proj, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "\n",
    "        branch_pool = self.branch_pool(x)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class GoogleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(GoogleNet, self).__init__()\n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # 使用AdaptiveAvgPool2d代替AvgPool2d\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layers(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.d4(x)\n",
    "        x = self.e4(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.b5(x)\n",
    "\n",
    "        x = self.avgpool(x)  # 使用AdaptiveAvgPool2d不需要手动设置kernel_size\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleNet(\n",
       "  (pre_layers): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (a3): Inception(\n",
       "    (branch1x1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3): Sequential(\n",
       "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (branch5x5): Sequential(\n",
       "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (branch_pool): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (b3): Inception(\n",
       "    (branch1x1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (branch5x5): Sequential(\n",
       "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (branch_pool): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (a4): Inception(\n",
       "    (branch1x1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3): Sequential(\n",
       "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (branch5x5): Sequential(\n",
       "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (branch_pool): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (b4): Inception(\n",
       "    (branch1x1): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3): Sequential(\n",
       "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (branch5x5): Sequential(\n",
       "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (branch_pool): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (c4): Inception(\n",
       "    (branch1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (branch5x5): Sequential(\n",
       "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (branch_pool): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (d4): Inception(\n",
       "    (branch1x1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3): Sequential(\n",
       "      (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (branch5x5): Sequential(\n",
       "      (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (branch_pool): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (e4): Inception(\n",
       "    (branch1x1): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3): Sequential(\n",
       "      (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (branch5x5): Sequential(\n",
       "      (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (branch_pool): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (a5): Inception(\n",
       "    (branch1x1): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3): Sequential(\n",
       "      (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (branch5x5): Sequential(\n",
       "      (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (branch_pool): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (b5): Inception(\n",
       "    (branch1x1): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3): Sequential(\n",
       "      (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (branch5x5): Sequential(\n",
       "      (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (branch_pool): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GoogleNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP, gamma=GAMMA)\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# 定义warm-up学习率调度器\n",
    "def warmup_lr_scheduler(optimizer, warmup_epochs, multiplier):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (multiplier - 1.0) * epoch / warmup_epochs + 1.0\n",
    "        return 1.0\n",
    "\n",
    "    return lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# 在训练代码中引入warm-up学习率调度器\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "warmup_epochs = NUM_EPOCHS//5\n",
    "multiplier = 10\n",
    "scheduler = warmup_lr_scheduler(optimizer, warmup_epochs, multiplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: per batch training/testing\n",
    "---\n",
    "\n",
    "Please denfine two function named ``train_batch`` and ``test_batch``. These functions are essential for training and evaluating machine learning models using batched data from dataloaders.\n",
    "\n",
    "**To do**: \n",
    "1. Define the loss function i.e [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "2. Take the image as the input and generate the output using the pre-defined SimpleNet.\n",
    "3. Calculate the loss between the output and the corresponding label using the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, image, target):\n",
    "    \"\"\"\n",
    "    Perform one training batch iteration.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The machine learning model to train.\n",
    "        image (torch.Tensor): Batch of input data (images).\n",
    "        target (torch.Tensor): Batch of target labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Model output (predictions) for the batch.\n",
    "        torch.Tensor: Loss value calculated by the defined loss function loss_fn().\n",
    "    \"\"\"\n",
    "    \n",
    "    ##################### Write your answer here ##################\n",
    "    #model.train()\n",
    "    #optimizer.zero_grad()\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "    #loss.backward()\n",
    "    #optimizer.step()\n",
    "\n",
    "    ###############################################################\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Write your answer here ##################\n",
    "# Define the loss function\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=1.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        y_pred: raw scores (logits) for each class. [batch_size, num_classes]\n",
    "        y_true: ground truth labels. [batch_size]\n",
    "        \"\"\"\n",
    "        # Convert y_true labels into one-hot encoding\n",
    "        y_true_onehot = torch.zeros(y_pred.size(), device=y_pred.device).scatter_(1, y_true.unsqueeze(1).long(), 1)\n",
    "        \n",
    "        # Calculate softmax over y_pred for calculating probabilities\n",
    "        probs = F.softmax(y_pred, dim=1)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        focal_loss = -self.alpha * (y_true_onehot * torch.log(probs) * (1 - probs) ** self.gamma).sum(dim=1).mean()\n",
    "        \n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "#criterion = nn.L1Loss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = FocalLoss(gamma=0.5)\n",
    "#criterion = FocalLoss(gamma=2.0)\n",
    "\n",
    "def one_hot_encoding(target, num_classes):\n",
    "    return F.one_hot(target, num_classes=num_classes).float()\n",
    "\n",
    "\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_batch(model, image, target):\n",
    "    \"\"\"\n",
    "    Perform one testing batch iteration.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The machine learning model to evaluate.\n",
    "        image (torch.Tensor): Batch of input data (images).\n",
    "        target (torch.Tensor): Batch of target labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Model output (predictions) for the batch.\n",
    "        torch.Tensor: Loss value calculated for the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    ##################### Write your answer here ##################\n",
    "   # model.eval()\n",
    "   # with torch.no_grad():\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "        \n",
    "        \n",
    "    ###############################################################\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300 Train Loss: 0.0180 Acc: 0.1002\n",
      "f1 score in 0th epoch is 0.032191261119832545\n",
      "Begin test......\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/opt/pytorch/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1150, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 84\u001b[0m\n\u001b[1;32m     79\u001b[0m   target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     81\u001b[0m   \u001b[38;5;66;03m# test model\u001b[39;00m\n\u001b[1;32m     82\u001b[0m   \u001b[38;5;66;03m##################\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#  target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m   outputs, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtest_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#  outputs, loss = test_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\u001b[39;00m\n\u001b[1;32m     86\u001b[0m   \u001b[38;5;66;03m#######################\u001b[39;00m\n\u001b[1;32m     87\u001b[0m   _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mtest_batch\u001b[0;34m(model, image, target)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m Perform one testing batch iteration.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m     torch.Tensor: Loss value calculated for the batch.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m \"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m  \u001b[38;5;66;03m##################### Write your answer here ##################\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m  output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m  loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m     22\u001b[0m  \u001b[38;5;66;03m###############################################################\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m, in \u001b[0;36mGoogleNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma3(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb3(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/opt/pytorch/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1150, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "training_loss = []\n",
    "training_acc = []\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "start_time = time.time()  # 记录开始时间\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "# 日志文件\n",
    "log_file = 'training_log.txt'\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ##########################\n",
    "    ### Training\n",
    "    ##########################\n",
    "\n",
    "    running_cls_loss = 0.0\n",
    "    running_cls_corrects = 0\n",
    "\n",
    "    for batch_idx, (image, target) in enumerate(train_dataloader):\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        #######################\n",
    "        # 为使用L1loss function：\n",
    "       # target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\n",
    "\n",
    "\n",
    "        # train model\n",
    "       # outputs, loss = train_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\n",
    "        \n",
    "        outputs, loss = train_batch(model, image, target)\n",
    "        #########################\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        \n",
    "        loss_data = loss.data.item()\n",
    "        if np.isnan(loss_data):\n",
    "            raise ValueError('loss is nan while training')\n",
    "        running_cls_loss += loss.item()\n",
    "        running_cls_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = running_cls_loss / len(train_set)\n",
    "    epoch_acc = running_cls_corrects.double() / len(train_set)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "    #########\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_targets.extend(target.cpu().numpy())\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    print(f\"f1 score in {epoch}th epoch is {f1}\")\n",
    "\n",
    "############\n",
    "    training_loss.append(epoch_loss)\n",
    "    training_acc.append(epoch_acc.cpu().detach().numpy())\n",
    "\n",
    "    # change learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    ### Testing\n",
    "    ##########################\n",
    "    # # eval model during training or in the last epoch\n",
    "    \n",
    "    if (epoch + 1) % EVAL_INTERVAL == 0 or (epoch +1) == NUM_EPOCHS:\n",
    "        print('Begin test......')\n",
    "        model.eval()\n",
    "    \n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # test model\n",
    "            ##################\n",
    "          #  target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\n",
    "            outputs, loss = test_batch(model, image, target)\n",
    "          #  outputs, loss = test_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\n",
    "            #######################\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_set)\n",
    "        val_acc = val_corrects.double() / len(test_set)\n",
    "        print(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        testing_loss.append(val_loss)\n",
    "        testing_acc.append(val_acc.cpu().detach().numpy())\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f'Epoch {epoch+1}/{NUM_EPOCHS}\\n')\n",
    "        f.write(f'Train Accuracy: {epoch_acc:.4f}%\\n')\n",
    "        f.write(f'Test Accuracy: {val_acc:.4f}%\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "        # save the model in last epoch\n",
    "        if (epoch +1) == NUM_EPOCHS:\n",
    "            \n",
    "            state = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': epoch_acc,\n",
    "            'epoch': (epoch+1),\n",
    "            }\n",
    "\n",
    "            # check the dir\n",
    "            if not os.path.exists(SAVE_DIR):\n",
    "                os.makedirs(SAVE_DIR)\n",
    "\n",
    "            # save the state\n",
    "            torch.save(state, osp.join(SAVE_DIR, 'checkpoint_%s.pth' % (str(epoch+1))))\n",
    "            \n",
    "end_time = time.time()  # 记录结束时间\n",
    "duration = end_time - start_time  # 计算训练时间\n",
    "print(f\"训练模型用时：{duration}秒\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Instance inference\n",
    "---\n",
    "The task is to visualizes an image along with model prediction and class probabilities.\n",
    "\n",
    "**To do**: \n",
    "1. Calculate the prediction and the probabilities for each class.\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(test_dataloader))\n",
    "input = inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Write your answer here ##################\n",
    "# input: image, model\n",
    "# outputs: predict_label, probabilities\n",
    "# predict_label is the index (or label) of the class with the highest probability from the probabilities.\n",
    "###############################################################\n",
    "input_tensor = input.unsqueeze(0).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    probabilities = F.softmax(output, dim=1)\n",
    "    predict_label = torch.argmax(probabilities)\n",
    "# predict_label = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = class_names[predict_label.item()]\n",
    "predicted_probability = probabilities[0][predict_label.item()]\n",
    "image = input.numpy().transpose((1, 2, 0))\n",
    "plt.imshow(image)\n",
    "plt.text(17, 30, f'Predicted Class: {predicted_class}\\nProbability: {predicted_probability:.2f}', \n",
    "            color='white', backgroundcolor='black', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# Print probabilities for each class\n",
    "print('Print probabilities for each class:')\n",
    "for i in range(len(class_names)):\n",
    "    print(f'{class_names[i]}: {probabilities[0][i].item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.array(training_acc):\n",
    "    print(f\"{i:.5f}\",end=',')\n",
    "print(\"\\n\\n\")\n",
    "for i in np.array(testing_acc):\n",
    "    print(f\"{i:.5f}\",end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "epochs = list(range(1, 301))\n",
    "line1 = ax.plot(epochs, np.array(training_acc), label=\"train acc\")\n",
    "line2 = ax.plot(epochs, np.array(testing_acc), label=\"test acc\")\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.grid(True)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
