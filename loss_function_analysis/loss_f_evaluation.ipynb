{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports | Hyperparameters | Device | Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "## Hyperparameters \n",
    "# random seed\n",
    "SEED = 1 \n",
    "NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 30\n",
    "EVAL_INTERVAL=1\n",
    "SAVE_DIR = './log'\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-1\n",
    "MOMENTUM = 0.9\n",
    "STEP=5\n",
    "GAMMA=0.5\n",
    "\n",
    "## Device\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## Dataset\n",
    "# cifar10 transform\n",
    "transform_cifar10_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_cifar10_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='../../data', train=True,\n",
    "                                        download=True, transform=transform_cifar10_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='../../data', train=False,\n",
    "                                       download=True, transform=transform_cifar10_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model | Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model def\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 4, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3)\n",
    "        self.fc1 = nn.Linear(8 * 6 * 6, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 6 * 6)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "## Init model\n",
    "model = ConvNet()\n",
    "model.to(device)\n",
    "\n",
    "## Init Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Modify here ###########################\n",
    "# Define the loss function\n",
    "criterion = nn.L1Loss()\n",
    "###############################################################\n",
    "\n",
    "## Define the batch train\n",
    "def train_batch(model, image, target):\n",
    "    \"\"\"\n",
    "    Perform one training batch iteration.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The machine learning model to train.\n",
    "        image (torch.Tensor): Batch of input data (images).\n",
    "        target (torch.Tensor): Batch of target labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Model output (predictions) for the batch.\n",
    "        torch.Tensor: Loss value calculated by the defined loss function loss_fn().\n",
    "    \"\"\"\n",
    "    \n",
    "    ####################### Modify here ###########################\n",
    "    output = model(image)\n",
    "    loss = criterion(F.softmax(output, dim=1), F.one_hot(target, num_classes=10))\n",
    "    ###############################################################\n",
    "\n",
    "    return output, loss\n",
    "\n",
    "\n",
    "def test_batch(model, image, target):\n",
    "    \"\"\"\n",
    "    Perform one testing batch iteration.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The machine learning model to evaluate.\n",
    "        image (torch.Tensor): Batch of input data (images).\n",
    "        target (torch.Tensor): Batch of target labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Model output (predictions) for the batch.\n",
    "        torch.Tensor: Loss value calculated for the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    ####################### Modify here ###########################\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        loss = criterion(F.softmax(output, dim=1), F.one_hot(target, num_classes=10))\n",
    "    ###############################################################\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30\n",
      "Begin test......\n",
      "Test Loss: 0.0013 Acc: 0.1707 Precision: 0.0848 Recall: 0.1707 f1score_micro: 0.1707 f1score_macro: 0.0780\n",
      "Epoch: 2/30\n",
      "Begin test......\n",
      "Test Loss: 0.0012 Acc: 0.2119 Precision: 0.1266 Recall: 0.2119 f1score_micro: 0.2119 f1score_macro: 0.1444\n",
      "Epoch: 3/30\n",
      "Begin test......\n",
      "Test Loss: 0.0012 Acc: 0.2362 Precision: 0.1505 Recall: 0.2362 f1score_micro: 0.2362 f1score_macro: 0.1684\n",
      "Epoch: 4/30\n",
      "Begin test......\n",
      "Test Loss: 0.0012 Acc: 0.2422 Precision: 0.1413 Recall: 0.2422 f1score_micro: 0.2422 f1score_macro: 0.1705\n",
      "Epoch: 5/30\n",
      "Begin test......\n",
      "Test Loss: 0.0012 Acc: 0.2597 Precision: 0.1427 Recall: 0.2597 f1score_micro: 0.2597 f1score_macro: 0.1776\n",
      "Epoch: 6/30\n",
      "Begin test......\n",
      "Test Loss: 0.0012 Acc: 0.2730 Precision: 0.1517 Recall: 0.2730 f1score_micro: 0.2730 f1score_macro: 0.1890\n",
      "Epoch: 7/30\n",
      "Begin test......\n",
      "Test Loss: 0.0012 Acc: 0.2679 Precision: 0.1623 Recall: 0.2679 f1score_micro: 0.2679 f1score_macro: 0.1896\n",
      "Epoch: 8/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.2798 Precision: 0.1484 Recall: 0.2798 f1score_micro: 0.2798 f1score_macro: 0.1902\n",
      "Epoch: 9/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.2771 Precision: 0.1570 Recall: 0.2771 f1score_micro: 0.2771 f1score_macro: 0.1903\n",
      "Epoch: 10/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.2894 Precision: 0.1523 Recall: 0.2894 f1score_micro: 0.2894 f1score_macro: 0.1973\n",
      "Epoch: 11/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.2983 Precision: 0.1632 Recall: 0.2983 f1score_micro: 0.2983 f1score_macro: 0.2046\n",
      "Epoch: 12/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3041 Precision: 0.1673 Recall: 0.3041 f1score_micro: 0.3041 f1score_macro: 0.2095\n",
      "Epoch: 13/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.2993 Precision: 0.1659 Recall: 0.2993 f1score_micro: 0.2993 f1score_macro: 0.2069\n",
      "Epoch: 14/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3078 Precision: 0.1721 Recall: 0.3078 f1score_micro: 0.3078 f1score_macro: 0.2114\n",
      "Epoch: 15/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3112 Precision: 0.1729 Recall: 0.3112 f1score_micro: 0.3112 f1score_macro: 0.2138\n",
      "Epoch: 16/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3090 Precision: 0.1707 Recall: 0.3090 f1score_micro: 0.3090 f1score_macro: 0.2125\n",
      "Epoch: 17/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3147 Precision: 0.1777 Recall: 0.3147 f1score_micro: 0.3147 f1score_macro: 0.2176\n",
      "Epoch: 18/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3117 Precision: 0.1685 Recall: 0.3117 f1score_micro: 0.3117 f1score_macro: 0.2133\n",
      "Epoch: 19/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3123 Precision: 0.1716 Recall: 0.3123 f1score_micro: 0.3123 f1score_macro: 0.2148\n",
      "Epoch: 20/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3157 Precision: 0.1754 Recall: 0.3157 f1score_micro: 0.3157 f1score_macro: 0.2178\n",
      "Epoch: 21/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3188 Precision: 0.1724 Recall: 0.3188 f1score_micro: 0.3188 f1score_macro: 0.2192\n",
      "Epoch: 22/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3187 Precision: 0.1722 Recall: 0.3187 f1score_micro: 0.3187 f1score_macro: 0.2187\n",
      "Epoch: 23/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3191 Precision: 0.1763 Recall: 0.3191 f1score_micro: 0.3191 f1score_macro: 0.2201\n",
      "Epoch: 24/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3171 Precision: 0.1736 Recall: 0.3171 f1score_micro: 0.3171 f1score_macro: 0.2181\n",
      "Epoch: 25/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3196 Precision: 0.1800 Recall: 0.3196 f1score_micro: 0.3196 f1score_macro: 0.2218\n",
      "Epoch: 26/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3194 Precision: 0.1726 Recall: 0.3194 f1score_micro: 0.3194 f1score_macro: 0.2192\n",
      "Epoch: 27/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3194 Precision: 0.1773 Recall: 0.3194 f1score_micro: 0.3194 f1score_macro: 0.2207\n",
      "Epoch: 28/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3203 Precision: 0.1750 Recall: 0.3203 f1score_micro: 0.3203 f1score_macro: 0.2206\n",
      "Epoch: 29/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3200 Precision: 0.1726 Recall: 0.3200 f1score_micro: 0.3200 f1score_macro: 0.2193\n",
      "Epoch: 30/30\n",
      "Begin test......\n",
      "Test Loss: 0.0011 Acc: 0.3195 Precision: 0.1736 Recall: 0.3195 f1score_micro: 0.3195 f1score_macro: 0.2194\n"
     ]
    }
   ],
   "source": [
    "test_loss_l = []\n",
    "test_acc_l = []\n",
    "test_precision_l = []\n",
    "test_recall_l = []\n",
    "test_f1score_micro_l = []\n",
    "test_f1score_macro_l = []\n",
    "\n",
    "test_acc = torchmetrics.classification.MulticlassAccuracy(num_classes=10).to(device)\n",
    "test_precision = torchmetrics.classification.MulticlassPrecision(num_classes=10, average='macro').to(device)\n",
    "test_recall = torchmetrics.classification.MulticlassRecall(num_classes=10, average='macro').to(device)\n",
    "test_f1score_micro = torchmetrics.classification.MulticlassF1Score(num_classes=10, average='micro').to(device)\n",
    "test_f1score_macro = torchmetrics.classification.MulticlassF1Score(num_classes=10, average='macro').to(device)\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ##########################\n",
    "    ### Training\n",
    "    ##########################\n",
    "\n",
    "    for batch_idx, (image, target) in enumerate(train_dataloader):\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # train model\n",
    "        outputs, loss = train_batch(model, image, target)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        \n",
    "        loss_data = loss.data.item()\n",
    "        if np.isnan(loss_data):\n",
    "            raise ValueError('loss is nan while training')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS}')\n",
    "\n",
    "    # change learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    ##########################\n",
    "    ### Testing\n",
    "    ##########################\n",
    "    # # eval model during training or in the last epoch\n",
    "    if (epoch + 1) % EVAL_INTERVAL == 0 or (epoch +1) == NUM_EPOCHS:\n",
    "        print('Begin test......')\n",
    "        model.eval()\n",
    "        \n",
    "        test_loss = .0        \n",
    "        test_acc.reset()\n",
    "        test_precision.reset()    \n",
    "        test_recall.reset()\n",
    "        test_f1score_micro.reset()\n",
    "        test_f1score_macro.reset()\n",
    "\n",
    "        for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # test model\n",
    "            outputs, loss = test_batch(model, image, target)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            test_acc.update(preds, target)\n",
    "            test_precision.update(preds, target)\n",
    "            test_recall.update(preds, target)\n",
    "            test_f1score_micro.update(preds, target)\n",
    "            test_f1score_macro.update(preds, target)\n",
    "\n",
    "        val_loss = test_loss / len(test_set)\n",
    "        val_acc = test_acc.compute()\n",
    "        val_precision = test_precision.compute()\n",
    "        val_recall = test_recall.compute()\n",
    "        val_f1score_micro = test_f1score_micro.compute()\n",
    "        val_f1score_macro = test_f1score_macro.compute()\n",
    "        print(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f} Precision: {val_precision:.4f} Recall: {val_recall:.4f} f1score_micro: {val_f1score_micro:.4f} f1score_macro: {val_f1score_macro:.4f}')\n",
    "\n",
    "        test_loss_l.append(test_loss)\n",
    "        test_acc_l.append(val_acc.cpu().detach().numpy())\n",
    "        test_precision_l.append(val_precision.cpu().detach().numpy())\n",
    "        test_recall_l.append(val_recall.cpu().detach().numpy())\n",
    "        test_f1score_micro_l.append(val_f1score_micro.cpu().numpy())\n",
    "        test_f1score_macro_l.append(val_f1score_macro.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Invetigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6    5000\n",
       " 9    5000\n",
       " 4    5000\n",
       " 1    5000\n",
       " 2    5000\n",
       " 7    5000\n",
       " 8    5000\n",
       " 3    5000\n",
       " 5    5000\n",
       " 0    5000\n",
       " dtype: int64,\n",
       " 3    1000\n",
       " 8    1000\n",
       " 0    1000\n",
       " 6    1000\n",
       " 1    1000\n",
       " 9    1000\n",
       " 5    1000\n",
       " 7    1000\n",
       " 4    1000\n",
       " 2    1000\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets = pd.Series(train_set.targets)\n",
    "test_targets = pd.Series(test_set.targets)\n",
    "train_targets.value_counts(), test_targets.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
